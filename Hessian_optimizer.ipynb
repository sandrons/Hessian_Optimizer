{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxLj7fcmp1c1mRamtXO51k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandrons/Hessian_Optimizer/blob/main/Hessian_optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) 2024 Alessandro Temperoni\n"
      ],
      "metadata": {
        "id": "qZobFLp6LyC4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bheAcPOk2WP9"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization,Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape, Add, Dropout\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.python.keras.engine import data_adapter\n",
        "from tensorflow.python.eager import backprop\n",
        "from tensorflow.python.util import compat\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.eager import def_function\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.keras import backend_config\n",
        "from tensorflow.python.keras.optimizer_v2 import optimizer_v2\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import control_flow_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import state_ops\n",
        "from tensorflow.python.training import gen_training_ops\n",
        "from tensorflow.python.util.tf_export import keras_export"
      ],
      "metadata": {
        "id": "DvftTWPj8tMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "woJ1DFE8K4_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d4b1ff4-efc5-4122-d2f2-88af98bdb057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images, test_images = train_images / 255.0, test_images / 255.0"
      ],
      "metadata": {
        "id": "eLW--0k9Kt6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JZSRrCyLNCy",
        "outputId": "f8ac8507-e128-4597-9103-72c9386f7a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmBbTbTTLPjO",
        "outputId": "cee53b53-a250-4b9b-e23b-70340d40c9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hessian"
      ],
      "metadata": {
        "id": "f0RJNJoOxFvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Hessian(keras.optimizers.Optimizer):\n",
        "    def __init__(self,\n",
        "               learning_rate=0.005,\n",
        "               beta_1=0.9,\n",
        "               beta_2=0.999,\n",
        "               epsilon=1e-3,\n",
        "               amsgrad=False,\n",
        "               name='Hessian',\n",
        "               **kwargs):\n",
        "        super().__init__(name, **kwargs)\n",
        "        self._set_hyper('learning_rate', learning_rate)\n",
        "        self._set_hyper('decay', self._initial_decay)\n",
        "        self._set_hyper('beta_1', beta_1)\n",
        "        self._set_hyper('beta_2', beta_2)\n",
        "        self.epsilon = epsilon or backend_config.epsilon()\n",
        "        self.amsgrad = amsgrad\n",
        "        self.t = 0.00\n",
        "\n",
        "    def _create_slots(self, var_list):\n",
        "        \"\"\"For each model variable, create the optimizer variable associated with it.\n",
        "        TensorFlow calls these optimizer variables \"slots\".\n",
        "        For momentum optimization, we need one momentum slot per model variable.\n",
        "        \"\"\"\n",
        "        for var in var_list:\n",
        "            self.add_slot(var, \"m\") #previous variable i.e. weight or bias\n",
        "        for var in var_list:\n",
        "            self.add_slot(var, \"u\") #previous gradient\n",
        "\n",
        "    @tf.function\n",
        "    def _resource_apply_dense(self, grad, v, apply_state=None):\n",
        "\n",
        "        t = self.t + 1.0\n",
        "        var_device, var_dtype = v.device, v.dtype.base_dtype\n",
        "        coefficients = ((apply_state or {}).get((var_device, var_dtype))\n",
        "                        or self._fallback_apply_state(var_device, var_dtype))\n",
        "        update_ops = []\n",
        "        i = 0\n",
        "        g, diagonal = grad\n",
        "        m = self.get_slot(v, 'm')\n",
        "        u = self.get_slot(v, 'u')\n",
        "        beta1=coefficients['beta_1_t']\n",
        "        beta2=coefficients['beta_2_t']\n",
        "        epsilon=coefficients['epsilon']\n",
        "        lr=coefficients['lr_t']\n",
        "\n",
        "        m.assign(beta1*m + (1-beta1)*g)\n",
        "        if diagonal is None:\n",
        "            u.assign(beta2*u + (1-beta2)*g*g)\n",
        "        else:\n",
        "            u.assign(beta2*u + (1-beta2)*diagonal*diagonal)\n",
        "\n",
        "        m_hat = m/(1-tf.pow(beta1,t))\n",
        "        u_hat = u/(1-tf.pow(beta2,t))\n",
        "        update = -lr*m_hat/(tf.sqrt(u_hat) + epsilon)\n",
        "        update_ops.append(v.assign_add(update))\n",
        "\n",
        "        i = i + 1\n",
        "\n",
        "        m.assign(m)\n",
        "        u.assign(u)\n",
        "        v.assign(v)\n",
        "\n",
        "        tf.group(*update_ops)\n",
        "        return\n",
        "\n",
        "    def _resource_apply_sparse(self, grad, var):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {\n",
        "            **base_config,\n",
        "            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
        "        }\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        params = self.weights\n",
        "        # If the weights are generated by Keras V1 optimizer, it includes vhats\n",
        "        # even without amsgrad, i.e, V1 optimizer has 3x + 1 variables, while V2\n",
        "        # optimizer has 2x + 1 variables. Filter vhats out for compatibility.\n",
        "        num_vars = int((len(params) - 1) / 2)\n",
        "        if len(weights) == 3 * num_vars + 1:\n",
        "          weights = weights[:len(params)]\n",
        "        super(Hessian, self).set_weights(weights)\n",
        "\n",
        "    def _prepare_local(self, var_device, var_dtype, apply_state):\n",
        "        super(Hessian, self)._prepare_local(var_device, var_dtype, apply_state)\n",
        "        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n",
        "        beta_1_t = array_ops.identity(self._get_hyper('beta_1', var_dtype))\n",
        "        beta_2_t = array_ops.identity(self._get_hyper('beta_2', var_dtype))\n",
        "        beta_1_power = math_ops.pow(beta_1_t, local_step)\n",
        "        beta_2_power = math_ops.pow(beta_2_t, local_step)\n",
        "        lr = (apply_state[(var_device, var_dtype)]['lr_t'] *\n",
        "              (math_ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)))\n",
        "        apply_state[(var_device, var_dtype)].update(\n",
        "            dict(\n",
        "                lr=lr,\n",
        "                epsilon=ops.convert_to_tensor(\n",
        "                    self.epsilon, var_dtype),\n",
        "                beta_1_t=beta_1_t,\n",
        "                beta_1_power=beta_1_power,\n",
        "                one_minus_beta_1_t=1 - beta_1_t,\n",
        "                beta_2_t=beta_2_t,\n",
        "                beta_2_power=beta_2_power,\n",
        "                one_minus_beta_2_t=1 - beta_2_t))\n",
        "\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Hessian, self).get_config()\n",
        "        config.update({\n",
        "            'alpha': self._serialize_hyperparameter('learning_rate'),\n",
        "            'beta1': self._serialize_hyperparameter('beta_1'),\n",
        "            'beta2': self._serialize_hyperparameter('beta_2'),\n",
        "            'epsilon': self.epsilon\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "VN_ggE-D2fvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "AqgWeUNsxIEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class network(Model):\n",
        "    def __init__(self, X_test_d):\n",
        "        super(network, self).__init__()\n",
        "\n",
        "        self.conv1 = Conv1D(28, (3), activation='relu', input_shape=(28, 28))\n",
        "        self.max1 = MaxPooling1D((2))\n",
        "\n",
        "        self.conv3 = Conv1D(64, (3), activation='relu')\n",
        "        self.max2 = MaxPooling1D((2))\n",
        "\n",
        "        self.conv4 = Conv1D(64, (3), activation='relu')\n",
        "\n",
        "        self.flatten = Flatten()\n",
        "        self.dense = Dense(64, activation='relu')\n",
        "        self.dense1 = Dense(10, activation='softmax')\n",
        "\n",
        "    #@tf.function\n",
        "    def call(self, pair):\n",
        "\n",
        "        x = self.conv1(pair)\n",
        "        x = self.max1(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.max2(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.dense1(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def hvp3(self,var,vec,x,y):\n",
        "      # hessian-vector product; takes advantage of weighted gradient (hess is with respect to weight matrix)\n",
        "      # second derivative (on top)\n",
        "      with tf.GradientTape() as outer_tape:\n",
        "        # first derivative (inner)\n",
        "        with tf.GradientTape() as inner_tape:\n",
        "          logits = self(x, training=True)\n",
        "          loss = -self.compiled_loss(y,logits,regularization_losses=self.losses)\n",
        "        grads = inner_tape.gradient(loss,var)\n",
        "\n",
        "      hess_vec = outer_tape.gradient(grads,var,output_gradients=vec)\n",
        "      return hess_vec\n",
        "\n",
        "\n",
        "    #@tf.function\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        with backprop.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = self.compiled_loss(\n",
        "                y, y_pred, sample_weight=None, regularization_losses=self.losses)\n",
        "\n",
        "        #calculate accuracy\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        #compute diagonals\n",
        "        diagonals = []\n",
        "        for var in trainable_vars:\n",
        "          z = tf.random.normal(var.shape)\n",
        "          #Hessian-free method is an oracle to compute the multiplication between the H with a random vector z\n",
        "          Hz = self.hvp3(var,z,x,y)\n",
        "          #Hutchinson's method\n",
        "          diagonal = tf.multiply(Hz,z)\n",
        "          diagonals.append(diagonal)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(zip(gradients,diagonals), self.trainable_variables))\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "metadata": {
        "id": "SfN1jSc33w-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "U7--NzorxKE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS = 'accuracy'\n",
        "model = network(train_images)\n",
        "\n",
        "model.compile(optimizer=Hessian(learning_rate=0.005,epsilon=1e-3), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=METRICS)\n",
        "history = model.fit(train_images, train_labels, validation_split=0.2, callbacks=[\n",
        "        # patience: number of epochs with no improvement after which training will be stopped\n",
        "        EarlyStopping(monitor='val_loss', min_delta=.01, patience=5, mode='auto', verbose=0)],\n",
        "        batch_size=16,epochs=100)\n"
      ],
      "metadata": {
        "id": "JK1blBcB6aQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_images, train_labels, validation_split=0.2, callbacks=[\n",
        "        # patience: number of epochs with no improvement after which training will be stopped\n",
        "        EarlyStopping(monitor='val_loss', min_delta=.001, patience=5, mode='auto', verbose=0)]\n",
        "        ,batch_size=256,epochs=100)"
      ],
      "metadata": {
        "id": "d8R3KCxK8r5N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}